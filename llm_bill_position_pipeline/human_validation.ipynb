{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d91a2367-b3fa-4ff9-8141-02687ffbb080",
   "metadata": {},
   "source": [
    "# Human Validation for LLM annotation with 4 Class (Support, Oppose, Amend, and Monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05bde4a2-5ebf-4002-9b1c-f67ab0ab38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils import open_csv, save_csv\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515f121e-4717-4ec5-b50f-84802f74fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_validation_df(base_dir=\"human_validation\"):\n",
    "    bill_position_list = ['support', 'oppose', 'amend', 'monitor']\n",
    "    df_list = []\n",
    "\n",
    "    for bill_position in bill_position_list:\n",
    "        file_path = os.path.join(base_dir, f\"human_validation_{bill_position}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            if bill_position == 'mention':\n",
    "                df = df[90:200]\n",
    "                # df = df[:150]\n",
    "            # print(bill_position,len(df))\n",
    "            df = df.rename(columns={'final_ground_truth':'label'})\n",
    "            df_list.append(df)\n",
    "        else:\n",
    "            print(f\"Warning: File not found: {file_path}\")\n",
    "\n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No valid human validation CSV files found.\")\n",
    "\n",
    "\n",
    "def get_llm_bill_position_df():\n",
    "    llm_bill_position_df = open_csv('llm_output/llm_bill_position_final_df.csv')\n",
    "    return llm_bill_position_df\n",
    "\n",
    "\n",
    "def compare_labels(llm_df, human_df):\n",
    "    merged_df = pd.merge(\n",
    "        llm_df,\n",
    "        human_df,\n",
    "        on=[\"paragraph\", \"bill_id\"],\n",
    "        suffixes=(\"_llm\", \"_human\")\n",
    "    )\n",
    "\n",
    "    merged_df['label_match'] = merged_df['label_llm'] == merged_df['label_human']\n",
    "\n",
    "    # calcuate Accuracy \n",
    "    total_matches = merged_df['label_match'].sum()\n",
    "    total = len(merged_df)\n",
    "    overall_accuracy = total_matches / total if total > 0 else 0\n",
    "\n",
    "    print(f\"Overall Accuracy: {total_matches}/{total} = {overall_accuracy:.2%}\")\n",
    "\n",
    "    label_stats = merged_df.groupby('label_human').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'total_samples': len(x),\n",
    "            'correct_predictions': (x['label_llm'] == x['label_human']).sum(),\n",
    "            'accuracy': (x['label_llm'] == x['label_human']).mean()\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    print(\"\\nLabel-wise Accuracy Stats:\")\n",
    "    print(label_stats)\n",
    "\n",
    "    # calcuate F1 score \n",
    "    y_true = merged_df['label_human']\n",
    "    y_pred = merged_df['label_llm']\n",
    "    label_order = ['Support', 'Oppose', 'Amend', 'Monitor']\n",
    "\n",
    "    print(\"\\nClassification Report (includes precision, recall, F1-score):\")\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=label_order,\n",
    "        target_names=label_order,\n",
    "        digits=4\n",
    "    ))\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', labels=label_order)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', labels=label_order)\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "    print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5411f27b-9dd8-4922-a5a3-abc61c8d7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_validation_df = get_human_validation_df()\n",
    "llm_bill_position_df = get_llm_bill_position_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e106429-2c56-4097-a06d-13e57e382f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 379/391 = 96.93%\n",
      "\n",
      "Label-wise Accuracy Stats:\n",
      "  label_human  total_samples  correct_predictions  accuracy\n",
      "0       Amend           98.0                 92.0  0.938776\n",
      "1     Monitor          103.0                100.0  0.970874\n",
      "2      Oppose           98.0                 98.0  1.000000\n",
      "3     Support           92.0                 89.0  0.967391\n",
      "\n",
      "Classification Report (includes precision, recall, F1-score):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Support     0.9780    0.9674    0.9727        92\n",
      "      Oppose     0.9899    1.0000    0.9949        98\n",
      "       Amend     0.9787    0.9388    0.9583        98\n",
      "     Monitor     0.9524    0.9709    0.9615       103\n",
      "\n",
      "   micro avg     0.9743    0.9693    0.9718       391\n",
      "   macro avg     0.9748    0.9693    0.9719       391\n",
      "weighted avg     0.9744    0.9693    0.9717       391\n",
      "\n",
      "Macro F1 Score: 0.9719\n",
      "Micro F1 Score: 0.9718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1597850/3440807633.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  label_stats = merged_df.groupby('label_human').apply(\n"
     ]
    }
   ],
   "source": [
    "compare_labels(llm_bill_position_df, human_validation_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
