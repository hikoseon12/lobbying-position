{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19bab8f8-3af4-48ec-8bf3-97464fbe0d59",
   "metadata": {},
   "source": [
    "# Analysis 2: How Corporate Lobbying is related to Firm Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995b63a9-9da2-458a-9fe2-6768e7c9399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gammaln\n",
    "from joblib import Parallel, delayed\n",
    "from joblib.externals.loky.process_executor import TerminatedWorkerError\n",
    "import statsmodels.api as sm\n",
    "from utils import open_csv, save_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e0680-463e-4967-b5de-7983b3be566f",
   "metadata": {},
   "source": [
    "##### Load bill position data with firm size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3495c9b1-0f91-4881-9559-b5440a907653",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_year_df = open_csv('analysis_input/analysis2_firm_size_bill_position_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f9458-fe18-46fe-aed2-c1002732de8e",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f988ff7e-9d9b-4e1c-8c7e-123f711bef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_design_matrix(df: pd.DataFrame, predictors: List[str], vary_var: str,\n",
    "                         numeric_predictors: List[str] = None) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Create a design matrix from df using the list of predictors.\n",
    "    \n",
    "    Predictors in numeric_predictors (including vary_var) are treated as numeric;\n",
    "    all others are converted to categorical dummy variables.\n",
    "    An intercept column is added.\n",
    "    \"\"\"\n",
    "    if numeric_predictors is None:\n",
    "        numeric_predictors = [vary_var]\n",
    "    elif vary_var not in numeric_predictors:\n",
    "        numeric_predictors.append(vary_var)\n",
    "\n",
    "    X_parts = []\n",
    "    col_names = []\n",
    "    for col in predictors:\n",
    "        if col in numeric_predictors:\n",
    "            X_parts.append(df[[col]])\n",
    "            col_names.append(col)\n",
    "        else:\n",
    "            dummies = pd.get_dummies(df[col].astype('category'), prefix=col, drop_first=True)\n",
    "            X_parts.append(dummies)\n",
    "            col_names.extend(dummies.columns.tolist())\n",
    "    X_df = pd.concat(X_parts, axis=1)\n",
    "    X_df.insert(0, 'Intercept', 1)\n",
    "    return X_df, ['Intercept'] + col_names\n",
    "\n",
    "\n",
    "\n",
    "def dirichlet_neg_log_likelihood(theta: np.ndarray, X: np.ndarray, Y: np.ndarray,\n",
    "                                 k: int, p: int) -> float:\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for the Dirichlet regression model.\n",
    "    \"\"\"\n",
    "    beta = np.array(theta).reshape((k, p))\n",
    "    X_arr = np.array(X, dtype=np.float64)\n",
    "    Y_arr = np.array(Y, dtype=np.float64)\n",
    "    XB = np.dot(X_arr, beta.T)\n",
    "    alpha = np.exp(XB)\n",
    "    \n",
    "    if np.any(~np.isfinite(alpha)):\n",
    "        return np.inf\n",
    "        \n",
    "    sum_alpha = np.sum(alpha, axis=1)\n",
    "    ll = np.sum(gammaln(sum_alpha)) - np.sum(gammaln(alpha)) + np.sum((alpha - 1) * np.log(Y_arr))\n",
    "    return -ll\n",
    "\n",
    "\n",
    "def bootstrap_iteration(args: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    One bootstrap replicate for the Dirichlet regression QOI.\n",
    "    Returns delta_bs computed from the bootstrap sample.\n",
    "    \"\"\"\n",
    "    np.random.seed()\n",
    "\n",
    "    (df_dir, predictors, response_cols, cluster_var, initial_theta, k, p, num_clusters,\n",
    "     numeric_predictors, std_log_emp_10, std_log_emp_90, design_cols, vary_var, full_data) = args\n",
    "\n",
    "    sampled_clusters = np.random.choice(df_dir[cluster_var].unique(), size=num_clusters, replace=True)\n",
    "    boot_sample = pd.concat([df_dir[df_dir[cluster_var] == clust] for clust in sampled_clusters])\n",
    "    \n",
    "    X_boot_df, _ = create_design_matrix(boot_sample, predictors, vary_var, numeric_predictors=numeric_predictors)\n",
    "    X_boot_df = X_boot_df.reindex(columns=design_cols, fill_value=0).astype(float)\n",
    "    X_boot = X_boot_df.values\n",
    "    Y_boot = boot_sample[response_cols].values\n",
    "\n",
    "    try:\n",
    "        result_boot = minimize(dirichlet_neg_log_likelihood, initial_theta,\n",
    "                               args=(X_boot, Y_boot, k, p),\n",
    "                               method='L-BFGS-B',\n",
    "                               options={'maxfun': 50000, 'maxiter': 30000})\n",
    "        beta_boot = result_boot.x.reshape((k, p))\n",
    "    except Exception as e:\n",
    "        print(f\"Bootstrap error: {e}\")\n",
    "        return np.full(k, np.nan)\n",
    "    \n",
    "    if np.isnan(beta_boot).any():\n",
    "        return np.full(k, np.nan)\n",
    "\n",
    "    # Replace key predictor with standardized 10th and 90th values.\n",
    "    idx = design_cols.index(vary_var)\n",
    "    X_boot_10 = X_boot.copy()\n",
    "    X_boot_90 = X_boot.copy()\n",
    "    X_boot_10[:, idx] = std_log_emp_10\n",
    "    X_boot_90[:, idx] = std_log_emp_90\n",
    "\n",
    "    XB_10 = np.matmul(X_boot_10, beta_boot.T)\n",
    "    A10 = np.exp(XB_10)\n",
    "    props_10 = A10 / np.sum(A10, axis=1, keepdims=True)\n",
    "    avg_props_10 = np.mean(props_10, axis=0)\n",
    "\n",
    "    XB_90 = np.matmul(X_boot_90, beta_boot.T)\n",
    "    A90 = np.exp(XB_90)\n",
    "    props_90 = A90 / np.sum(A90, axis=1, keepdims=True)\n",
    "    avg_props_90 = np.mean(props_90, axis=0)\n",
    "\n",
    "    delta_bs = avg_props_90 - avg_props_10\n",
    "    return delta_bs\n",
    "\n",
    "\n",
    "def bootstrap_iteration_with_retry(args: tuple, max_retries: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Wrapper for bootstrap_iteration that retries if an error occurs.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return bootstrap_iteration(args)\n",
    "        except (TerminatedWorkerError, MemoryError, FloatingPointError, ValueError) as e:\n",
    "            print(f\"Error '{e}' on attempt {attempt + 1}; retrying...\")\n",
    "            time.sleep(2)\n",
    "    (df_dir, _, _, _, _, k, _, num_clusters,\n",
    "     _, _, _, _, vary_var, full_data) = args\n",
    "    return np.full(k, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896a219-743b-44ce-9240-dfe8ae98a58c",
   "metadata": {},
   "source": [
    "### Main Function: run_models_and_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b83126-29f0-45e1-80d0-1bffcb520c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models_and_plot(data: pd.DataFrame, predictors: List[str], vary_var: str,\n",
    "                        response_cols: List[str], logistic_response: str, cluster_var: str,\n",
    "                        num_bootstrap: int = 100, numeric_predictors: List[str] = None) -> Union[dict, None]:\n",
    "    \"\"\"\n",
    "    Runs Dirichlet and logistic regressions.\n",
    "    Generates a figure and CSV summaries for the QOI.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    if numeric_predictors is None:\n",
    "        numeric_predictors = [vary_var]\n",
    "    elif vary_var not in numeric_predictors:\n",
    "        numeric_predictors.append(vary_var)\n",
    "\n",
    "    # --- Prepare Dirichlet Data ---\n",
    "    dirichlet_cols = [cluster_var] + predictors + response_cols\n",
    "    df_dir = data[dirichlet_cols].dropna().copy()\n",
    "    for pred in numeric_predictors:\n",
    "        if pred in df_dir.columns:\n",
    "            mean_val = df_dir[pred].mean()\n",
    "            std_val = df_dir[pred].std()\n",
    "            if std_val > 0:\n",
    "                df_dir[pred] = (df_dir[pred] - mean_val) / std_val\n",
    "            print(f\"{pred} in Dirichlet subset: mean={mean_val:.4f}, std={std_val:.4f}\")\n",
    "\n",
    "    epsilon = 1e-6\n",
    "    df_dir[response_cols] = df_dir[response_cols] + epsilon\n",
    "    df_dir[response_cols] = df_dir[response_cols].div(df_dir[response_cols].sum(axis=1), axis=0)\n",
    "\n",
    "    # --- Dirichlet Regression: Create Design Matrix ---\n",
    "    X_design_df, design_cols = create_design_matrix(df_dir, predictors, vary_var, numeric_predictors=numeric_predictors)\n",
    "    X_design_df = X_design_df.astype(float)\n",
    "    X_design = X_design_df.values\n",
    "    n, p = X_design.shape\n",
    "    print(f\"Dirichlet design matrix dimensions: {n} rows x {p} columns\")\n",
    "    Y = df_dir[response_cols].values\n",
    "    k = Y.shape[1]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    initial_theta = np.random.normal(0, 0.1, k * p)\n",
    "\n",
    "    print(f\"Starting Dirichlet regression with {p} parameters per category, {k} categories\")\n",
    "    try:\n",
    "        result = minimize(dirichlet_neg_log_likelihood, initial_theta,\n",
    "                          args=(X_design, Y, k, p),\n",
    "                          method='L-BFGS-B',\n",
    "                          options={'maxfun': 50000, 'maxiter': 30000})\n",
    "        beta_hat = result.x.reshape((k, p))\n",
    "        print(\"Dirichlet Optimization success:\", result.success)\n",
    "        coef_df = pd.DataFrame(beta_hat, columns=design_cols, index=[f\"label{j+1}\" for j in range(k)])\n",
    "        print(\"\\nDirichlet Coefficients by Category:\")\n",
    "        print(coef_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Dirichlet regression: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Percentile Calculations for vary_var ---\n",
    "    log_emp_10 = np.percentile(df_dir[vary_var].dropna(), 10)\n",
    "    log_emp_90 = np.percentile(df_dir[vary_var].dropna(), 90)\n",
    "    print(f\"10th percentile of {vary_var} (Dirichlet subset): {log_emp_10}\")\n",
    "    print(f\"90th percentile of {vary_var} (Dirichlet subset): {log_emp_90}\")\n",
    "    std_log_emp_10 = log_emp_10\n",
    "    std_log_emp_90 = log_emp_90\n",
    "\n",
    "    # --- Compute Dirichlet QOI ---\n",
    "    idx = design_cols.index(vary_var)\n",
    "    X_design_10 = X_design.copy()\n",
    "    X_design_90 = X_design.copy()\n",
    "    X_design_10[:, idx] = std_log_emp_10\n",
    "    X_design_90[:, idx] = std_log_emp_90\n",
    "    \n",
    "    XB_10 = np.matmul(X_design_10, beta_hat.T)\n",
    "    A10 = np.exp(XB_10)\n",
    "    proportions_10 = A10 / np.sum(A10, axis=1, keepdims=True)\n",
    "    avg_props_10 = np.mean(proportions_10, axis=0)\n",
    "    \n",
    "    XB_90 = np.matmul(X_design_90, beta_hat.T)\n",
    "    A90 = np.exp(XB_90)\n",
    "    proportions_90 = A90 / np.sum(A90, axis=1, keepdims=True)\n",
    "    avg_props_90 = np.mean(proportions_90, axis=0)\n",
    "    \n",
    "    delta_dir = avg_props_90 - avg_props_10\n",
    "\n",
    "    # --- Parallel Bootstrapping with Checkpointing ---\n",
    "    unique_clusters = df_dir[cluster_var].unique()\n",
    "    num_clusters = len(unique_clusters)\n",
    "    all_boot_results = []\n",
    "    batch_size = 500  # Adjust as needed\n",
    "    num_batches = int(np.ceil(num_bootstrap / batch_size))\n",
    "\n",
    "    # Define arguments for bootstrap iterations\n",
    "    bs_args = (df_dir, predictors, response_cols, cluster_var,\n",
    "               initial_theta, k, p, num_clusters, numeric_predictors,\n",
    "               std_log_emp_10, std_log_emp_90, design_cols, vary_var, data)\n",
    "\n",
    "    for b in range(num_batches):\n",
    "        batch_dir = \"analysis_output/analysis2_batch_result\"\n",
    "        os.makedirs(batch_dir, exist_ok=True)\n",
    "        batch_filename = f\"{batch_dir}/batch_results_{b+1}.pkl\"\n",
    "        if os.path.exists(batch_filename):\n",
    "            print(f\"Batch {b+1} already exists; loading results.\")\n",
    "            with open(batch_filename, \"rb\") as f:\n",
    "                batch_results = pickle.load(f)\n",
    "            for i, result in enumerate(batch_results):\n",
    "                if np.all(np.isnan(result)):\n",
    "                    print(f\"Iteration {i+1} in batch {b+1} failed (NaN). Rerunning...\")\n",
    "                    batch_results[i] = bootstrap_iteration_with_retry(bs_args)\n",
    "            with open(batch_filename, \"wb\") as f:\n",
    "                pickle.dump(batch_results, f)\n",
    "            print(f\"Updated batch {b+1} results saved to {batch_filename}\")\n",
    "        else:\n",
    "            current_batch = min(batch_size, num_bootstrap - b * batch_size)\n",
    "            print(f\"Starting batch {b+1}/{num_batches} with {current_batch} iterations\")\n",
    "            batch_results = Parallel(n_jobs=-1, verbose=5)(\n",
    "                delayed(bootstrap_iteration_with_retry)(bs_args) for _ in range(current_batch)\n",
    "            )\n",
    "            with open(batch_filename, \"wb\") as f:\n",
    "                pickle.dump(batch_results, f)\n",
    "            print(f\"Saved batch {b+1} results to {batch_filename}\")\n",
    "        all_boot_results.extend(batch_results)\n",
    "\n",
    "    all_boot_results = np.array(all_boot_results)\n",
    "    boot_delta = np.vstack(all_boot_results)\n",
    "    se_dir = np.nanstd(boot_delta, axis=0, ddof=1)\n",
    "    \n",
    "    # --- Prepare Logistic Data ---\n",
    "    logit_cols = [cluster_var] + predictors + [logistic_response]\n",
    "    df_logit = data[logit_cols].dropna().copy()\n",
    "    for pred in numeric_predictors:\n",
    "        if pred in df_logit.columns:\n",
    "            mean_val = df_logit[pred].mean()\n",
    "            std_val = df_logit[pred].std()\n",
    "            if std_val > 0:\n",
    "                df_logit[pred] = (df_logit[pred] - mean_val) / std_val\n",
    "            print(f\"{pred} in Logistic subset: mean={mean_val:.4f}, std={std_val:.4f}\")\n",
    "            \n",
    "    # --- Logistic Regression ---\n",
    "    X_logit_df, logit_design_cols = create_design_matrix(df_logit, predictors, vary_var, numeric_predictors=numeric_predictors)\n",
    "    #########\n",
    "    # X_logit = X_logit_df.values\n",
    "    X_logit_df = X_logit_df.astype(float)\n",
    "    X_logit = X_logit_df.values\n",
    "    y_logit = df_logit[logistic_response].astype(float)\n",
    "    print(f\"Logistic design matrix dimensions: {X_logit.shape[0]} rows x {X_logit.shape[1]} columns\")\n",
    "    model_logit = sm.Logit(y_logit, X_logit)\n",
    "    # model_logit = sm.Logit(df_logit[logistic_response], X_logit)\n",
    "    #########\n",
    "    result_logit = model_logit.fit(disp=0, cov_type='cluster', cov_kwds={'groups': df_logit[cluster_var]})\n",
    "\n",
    "    # --- Logistic QOI Evaluation (Delta Method) ---\n",
    "    idx_logit = logit_design_cols.index(vary_var)\n",
    "    X_logit_10 = X_logit.copy()\n",
    "    X_logit_90 = X_logit.copy()\n",
    "    logit_log_emp_10 = np.percentile(df_logit[vary_var].dropna(), 10)\n",
    "    logit_log_emp_90 = np.percentile(df_logit[vary_var].dropna(), 90)\n",
    "    X_logit_10[:, idx_logit] = logit_log_emp_10\n",
    "    X_logit_90[:, idx_logit] = logit_log_emp_90\n",
    "    print(f\"10th percentile of {vary_var} (Logistic subset): {logit_log_emp_10:.4f}\")\n",
    "    print(f\"90th percentile of {vary_var} (Logistic subset): {logit_log_emp_90:.4f}\")\n",
    "    preds_10_logit = result_logit.predict(X_logit_10)\n",
    "    preds_90_logit = result_logit.predict(X_logit_90)\n",
    "    p10_logit = preds_10_logit.mean()\n",
    "    p90_logit = preds_90_logit.mean()\n",
    "    delta_logit = p90_logit - p10_logit\n",
    "\n",
    "    grads_10 = [p * (1 - p) * x for p, x in zip(preds_10_logit, X_logit_10)]\n",
    "    avg_grad_10 = np.mean(grads_10, axis=0)\n",
    "    grads_90 = [p * (1 - p) * x for p, x in zip(preds_90_logit, X_logit_90)]\n",
    "    avg_grad_90 = np.mean(grads_90, axis=0)\n",
    "    avg_grad_diff = avg_grad_90 - avg_grad_10\n",
    "    var_logit = np.dot(avg_grad_diff, np.dot(result_logit.cov_params(), avg_grad_diff))\n",
    "    se_logit = np.sqrt(var_logit)\n",
    "\n",
    "    # --- Generate Figure and Save Results ---\n",
    "    ci95_lower_logit = delta_logit - 1.96 * se_logit\n",
    "    ci95_upper_logit = delta_logit + 1.96 * se_logit\n",
    "    ci90_lower_logit = delta_logit - 1.645 * se_logit\n",
    "    ci90_upper_logit = delta_logit + 1.645 * se_logit\n",
    "\n",
    "    ci95_lower_dir = delta_dir - 1.96 * se_dir\n",
    "    ci95_upper_dir = delta_dir + 1.96 * se_dir\n",
    "    ci90_lower_dir = delta_dir - 1.645 * se_dir\n",
    "    ci90_upper_dir = delta_dir + 1.645 * se_dir\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 6), sharex=True,\n",
    "                                   gridspec_kw={'height_ratios': [0.8, 3]})\n",
    "    ax1.errorbar(delta_logit, 0,\n",
    "                 xerr=[[delta_logit - ci95_lower_logit], [ci95_upper_logit - delta_logit]],\n",
    "                 fmt='o', color='black', markersize=8, linewidth=1)\n",
    "    ax1.errorbar(delta_logit, 0,\n",
    "                 xerr=[[delta_logit - ci90_lower_logit], [ci90_upper_logit - delta_logit]],\n",
    "                 fmt='o', color='black', markersize=8, linewidth=3)\n",
    "    ax1.axvline(0, color='gray', linestyle='--')\n",
    "    ax1.set_yticks([0])\n",
    "    ax1.set_ylim(-0.3, 0.3)\n",
    "    ax1.set_yticklabels([\"Lobby\"], fontsize=16)\n",
    "    for tick in ax1.get_yticklabels():\n",
    "        tick.set_verticalalignment('center')\n",
    "\n",
    "    outcome_labels = ['Support', 'Oppose', 'Amend', 'Monitor']\n",
    "    color_dict = {\n",
    "        'Support': '#01897B',\n",
    "        'Oppose': '#E6447B',\n",
    "        'Amend': '#F7A42F',\n",
    "        'Monitor': '#A900FF'\n",
    "    }\n",
    "    y_positions = np.linspace(1, 2.5, num=len(delta_dir))\n",
    "    for i, label in enumerate(outcome_labels):\n",
    "        ax2.errorbar(delta_dir[i], y_positions[i],\n",
    "                     xerr=[[delta_dir[i] - ci95_lower_dir[i]], [ci95_upper_dir[i] - delta_dir[i]]],\n",
    "                     fmt='o', color=color_dict[label], markersize=8, linewidth=1)\n",
    "        ax2.errorbar(delta_dir[i], y_positions[i],\n",
    "                     xerr=[[delta_dir[i] - ci90_lower_dir[i]], [ci90_upper_dir[i] - delta_dir[i]]],\n",
    "                     fmt='o', color=color_dict[label], markersize=8, linewidth=3)\n",
    "    ax2.axvline(0, color='gray', linestyle='--')\n",
    "    ax2.set_yticks(y_positions)\n",
    "    ax2.set_ylim(0.7, 2.8)\n",
    "    ax2.set_yticklabels(outcome_labels, fontsize=16)\n",
    "    for tick in ax2.get_yticklabels():\n",
    "        tick.set_verticalalignment('center')\n",
    "    ax2.set_xlabel(\"Effect of Firm Size on Predicted Probability\\n(90th - 10th Percentile)\", fontsize=16, labelpad=15)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    fig_filename = \"analysis2_fig3.pdf\"\n",
    "    fig.savefig(f\"analysis_output/{fig_filename}\")\n",
    "\n",
    "    results = {\n",
    "        \"delta_dir\": delta_dir,\n",
    "        \"se_dir\": se_dir,\n",
    "        \"ci95_lower_dir\": ci95_lower_dir,\n",
    "        \"ci95_upper_dir\": ci95_upper_dir,\n",
    "        \"ci90_lower_dir\": ci90_lower_dir,\n",
    "        \"ci90_upper_dir\": ci90_upper_dir,\n",
    "        \"delta_logit\": delta_logit,\n",
    "        \"se_logit\": se_logit,\n",
    "        \"ci95_lower_logit\": ci95_lower_logit,\n",
    "        \"ci95_upper_logit\": ci95_upper_logit,\n",
    "        \"ci90_lower_logit\": ci90_lower_logit,\n",
    "        \"ci90_upper_logit\": ci90_upper_logit,\n",
    "        \"figure_filename\": fig_filename\n",
    "    }\n",
    "    plt.close(fig)\n",
    "\n",
    "    # --- Save CSV Summaries ---\n",
    "    df_logistic_results = pd.DataFrame([{\n",
    "        \"delta_logit\": results[\"delta_logit\"],\n",
    "        \"se_logit\": results[\"se_logit\"],\n",
    "        \"ci95_lower_logit\": results[\"ci95_lower_logit\"],\n",
    "        \"ci95_upper_logit\": results[\"ci95_upper_logit\"],\n",
    "        \"ci90_lower_logit\": results[\"ci90_lower_logit\"],\n",
    "        \"ci90_upper_logit\": results[\"ci90_upper_logit\"]\n",
    "    }])\n",
    "    save_csv('analysis_output/analysis2_fig3_results_logistic.csv', df_logistic_results)\n",
    "\n",
    "    dirichlet_rows = []\n",
    "    for i, label in enumerate(outcome_labels):\n",
    "        dirichlet_rows.append({\n",
    "            \"outcome\": label,\n",
    "            \"delta_dir\": results[\"delta_dir\"][i],\n",
    "            \"se_dir\": results[\"se_dir\"][i],\n",
    "            \"ci95_lower_dir\": results[\"ci95_lower_dir\"][i],\n",
    "            \"ci95_upper_dir\": results[\"ci95_upper_dir\"][i],\n",
    "            \"ci90_lower_dir\": results[\"ci90_lower_dir\"][i],\n",
    "            \"ci90_upper_dir\": results[\"ci90_upper_dir\"][i]\n",
    "        })\n",
    "    df_dirichlet_results = pd.DataFrame(dirichlet_rows)\n",
    "    save_csv('analysis_output/analysis2_fig3_results_dirichlet.csv', df_dirichlet_results)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Total execution time: {:.2f} seconds\".format(end_time - start_time))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a455b138-4297-4170-94d0-d4c23a1ab42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_emp in Dirichlet subset: mean=9.4903, std=2.0010\n",
      "Dirichlet design matrix dimensions: 3517 rows x 15 columns\n",
      "Starting Dirichlet regression with 15 parameters per category, 4 categories\n",
      "Dirichlet Optimization success: True\n",
      "\n",
      "Dirichlet Coefficients by Category:\n",
      "        Intercept   log_emp  year_2010  year_2011  year_2012  year_2013  \\\n",
      "label1  -2.464736  0.036178   0.025624   0.165421   0.123797   0.093115   \n",
      "label2  -2.711395  0.020351   0.007526   0.036833   0.005423   0.024292   \n",
      "label3  -2.366520  0.036530  -0.006189  -0.007177  -0.130526  -0.150930   \n",
      "label4  -1.955240  0.108055  -0.018825   0.045516   0.045847   0.061896   \n",
      "\n",
      "        year_2014  year_2015  year_2016  year_2017  year_2018  year_2019  \\\n",
      "label1   0.044423   0.131669   0.175535   0.095040   0.085424   0.075206   \n",
      "label2   0.021026   0.013254   0.005801  -0.003307  -0.033020  -0.034181   \n",
      "label3  -0.208076  -0.127704  -0.138376  -0.130432  -0.193586  -0.200254   \n",
      "label4   0.203645   0.151454   0.047728   0.061779  -0.029690  -0.102399   \n",
      "\n",
      "        year_2020  year_2021  year_2022  \n",
      "label1   0.033290   0.013787  -0.034282  \n",
      "label2  -0.031307  -0.036287  -0.005469  \n",
      "label3  -0.227095  -0.270352  -0.285879  \n",
      "label4   0.047084   0.167446   0.631494  \n",
      "10th percentile of log_emp (Dirichlet subset): -1.3312167735657592\n",
      "90th percentile of log_emp (Dirichlet subset): 1.186983919397475\n",
      "Starting batch 1/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1 results to analysis_output/analysis2_batch_result/batch_results_1.pkl\n",
      "Starting batch 2/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   52.9s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 2 results to analysis_output/analysis2_batch_result/batch_results_2.pkl\n",
      "Starting batch 3/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   53.8s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 3 results to analysis_output/analysis2_batch_result/batch_results_3.pkl\n",
      "Starting batch 4/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 4 results to analysis_output/analysis2_batch_result/batch_results_4.pkl\n",
      "Starting batch 5/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 5 results to analysis_output/analysis2_batch_result/batch_results_5.pkl\n",
      "Starting batch 6/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 6 results to analysis_output/analysis2_batch_result/batch_results_6.pkl\n",
      "Starting batch 7/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 7 results to analysis_output/analysis2_batch_result/batch_results_7.pkl\n",
      "Starting batch 8/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 8 results to analysis_output/analysis2_batch_result/batch_results_8.pkl\n",
      "Starting batch 9/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 9 results to analysis_output/analysis2_batch_result/batch_results_9.pkl\n",
      "Starting batch 10/10 with 500 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   52.5s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 10 results to analysis_output/analysis2_batch_result/batch_results_10.pkl\n",
      "log_emp in Logistic subset: mean=6.3576, std=2.7409\n",
      "Logistic design matrix dimensions: 90905 rows x 15 columns\n",
      "10th percentile of log_emp (Logistic subset): -1.3567\n",
      "90th percentile of log_emp (Logistic subset): 1.3115\n",
      "Total execution time: 812.79 seconds\n"
     ]
    }
   ],
   "source": [
    "results = run_models_and_plot(\n",
    "    data=client_year_df,\n",
    "    predictors=['log_emp', 'year'],\n",
    "    vary_var='log_emp',\n",
    "    response_cols=['ratio_label1', 'ratio_label2', 'ratio_label3', 'ratio_label4'],\n",
    "    logistic_response='lobbied',\n",
    "    cluster_var='firm_id',\n",
    "    num_bootstrap=5000,\n",
    "    numeric_predictors=['log_emp']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
